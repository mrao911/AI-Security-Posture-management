{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvgZFaBGuAp2CS88KHBjd7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrao911/AI-Security-Posture-management/blob/main/Threat_Posture_Management_tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmrlcdnOoiBf",
        "outputId": "a8a72dbf-ee26-4ace-d412-35a0f7d3a6e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose an option:\n",
            "1. Use Sample Data\n",
            "2. Upload Threat Data\n"
          ]
        }
      ],
      "source": [
        "# prompt: create a threat posture management tool to analyze AI security incidents and provide risk score, number of incidents, categorize them as critical, high, medium, low risks, remediation options. The tool should have threat data upload option\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "# Sample Threat Data (Replace with your actual data upload mechanism)\n",
        "data = {\n",
        "    'Incident ID': [1, 2, 3, 4, 5],\n",
        "    'Description': ['Unauthorized model access', 'Data poisoning attack', 'Model evasion', 'Inference poisoning', 'Adversarial example'],\n",
        "    'Severity': ['High', 'Critical', 'Medium', 'Low', 'High'],\n",
        "    'Impact': ['Data breach', 'Model degradation', 'Incorrect predictions', 'Minor performance drop', 'Misinformation'],\n",
        "    'Remediation': ['Implement stricter authentication', 'Input data validation', 'Retrain model with adversarial examples', 'Parameter tuning', 'Input sanitization']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "def calculate_risk_score(severity):\n",
        "  \"\"\"Calculates risk score based on severity.\"\"\"\n",
        "  if severity == 'Critical':\n",
        "    return 100\n",
        "  elif severity == 'High':\n",
        "    return 75\n",
        "  elif severity == 'Medium':\n",
        "    return 50\n",
        "  elif severity == 'Low':\n",
        "    return 25\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "def analyze_incidents(df):\n",
        "  \"\"\"Analyzes AI security incidents.\"\"\"\n",
        "\n",
        "  df['Risk Score'] = df['Severity'].apply(calculate_risk_score)\n",
        "\n",
        "  num_incidents = len(df)\n",
        "\n",
        "  risk_categories = df['Severity'].value_counts().to_dict()\n",
        "\n",
        "  print(f\"Total Number of Incidents: {num_incidents}\")\n",
        "  print(\"\\nRisk Category Breakdown:\")\n",
        "  for category, count in risk_categories.items():\n",
        "    print(f\"- {category}: {count} incidents\")\n",
        "\n",
        "  print(\"\\nIncident Details:\")\n",
        "  print(df)\n",
        "\n",
        "\n",
        "# ---  Threat Data Upload (using Google Colab files upload) ---\n",
        "def upload_threat_data():\n",
        "  global df\n",
        "  uploaded = files.upload()\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))\n",
        "    try:\n",
        "      df = pd.read_csv(fn)  # Assumes CSV, adjust if different format\n",
        "    except Exception as e:\n",
        "      print(f\"Error reading uploaded file: {e}\")\n",
        "      return None\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "# Main part\n",
        "print(\"Choose an option:\")\n",
        "print(\"1. Use Sample Data\")\n",
        "print(\"2. Upload Threat Data\")\n",
        "\n",
        "choice = input(\"Enter your choice (1 or 2): \")\n",
        "\n",
        "if choice == '1':\n",
        "  analyze_incidents(df)\n",
        "elif choice == '2':\n",
        "  df_uploaded = upload_threat_data()\n",
        "  if df_uploaded is not None:\n",
        "    analyze_incidents(df_uploaded)\n",
        "  else:\n",
        "    print(\"Could not process the uploaded file.\")\n",
        "else:\n",
        "  print(\"Invalid choice.\")"
      ]
    }
  ]
}